# -*- coding: utf-8 -*-
"""Copy of Real Estate & Consumer Discretionary

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17wW0FIp93u8CNdhOG9BTdUT8KbOD3DC9
"""

from pandas_datareader import data as web
import pandas_datareader as pdr
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
plt.style.use('fivethirtyeight')

# real estate
# AMT : American Tower Corp [1998.02~]
# BXP : Boston Properties [1997.06~]
# EQIX : Equinix [2000.08~]
# PLD : Prologis [1997.11~]
# CCI : crown castle international [1998.08~]

# ZG : Zillow -> NASDAQ

realEstate_assets = ['AMT','BXP','EQIX','PLD','CCI']

# Assign weights to the stocks.
weights = np.array([0.2,0.2,0.2,0.2,0.2])

# Get the portfolio starting date
stockStartDate = '2001-01-01'

# Get the portfolio ending date (I think fixing the end date is more easier )
# today = datetime.today().strftime('%Y-%m-%d')
# today

# two lines is to import the csv file need solution to import it
# from google.colab import drive
# drive.mount('/content/drive')

# create a dateframe to store the adjusted close price of the stocks
df = pd.DataFrame()

# Store the adjusted close price of the stock into the df

base_path = "/content/drive/[temp]/RealEstate/"
for stock in realEstate_assets:
  # df[stock] = web.DataReader(stock,data_source='yahoo',start = stockStartDate,end=today)['Adj Close']
  df[stock] = pd.read_csv(base_path+str(stock)+".csv")['Adj Close']
  print(stock)
  print(pd.read_csv(base_path+str(stock)+".csv").shape)

df

from google.colab import drive
drive.mount('/content/drive')

# Visually show the stock / portfolio
title = "Portfolio Adj. Close Price History"

# Get the stocks
my_stocks = df

# Create and plot the graph
for c in my_stocks.columns.values:
  plt.plot(my_stocks[c], label = c)

plt.title(title)
plt.xlabel('Date',fontsize = 18)
plt.ylabel('Adj. Price USD ($)', fontsize = 18)
plt.legend(my_stocks.columns.values, loc = 'upper left')
plt.show()

# daily changes
returns = df.pct_change().dropna()
my_stocks = returns
# Create and plot the graph
for c in my_stocks.columns.values:
  plt.plot(my_stocks[c], label = c)

plt.title(title)
plt.xlabel('Date',fontsize = 18)
plt.ylabel('Adj. Price USD ($)', fontsize = 18)
plt.legend(my_stocks.columns.values, loc = 'upper left')
plt.show()

cov_matrix_annual = returns.cov() * 252
cov_matrix_annual

# Calculate the portfolio variance
port_variance = np.dot( weights.T, np.dot(cov_matrix_annual, weights))
port_variance

# Calculate the portfolio volatility (standard deviation)
port_volatility = np.sqrt(port_variance)
port_volatility

# Calculate the annual portfolio return
portfolioSimpleAnnualReturn = np.sum(returns.mean() *weights) * 252
portfolioSimpleAnnualReturn

# Show the expected Annual return, volatility (risk), and variance

percent_var = str(round(port_variance,2)*100) + '%'
percent_vols = str(round(port_volatility,2)*100) + '%'
percent_ret = str(round(portfolioSimpleAnnualReturn,2) *100) + '%'
print("Expected annual Return : "+percent_ret)
print("Annual volatility / risk : "+percent_vols)
print("Annual variance : "+percent_var)

!pip install PyPortfolioOpt

from pypfopt.efficient_frontier import EfficientFrontier
from pypfopt import risk_models
from pypfopt import expected_returns

"""
Expected annual Return : 21.0%
Annual volatility / risk : 31.0%
Annual variance : 9.0%
"""
# Portfolio Optimization
# Calculate the expected returns and the annualized sample covariance matrix of asset returns
mu = expected_returns.mean_historical_return(df)
S = risk_models.sample_cov(df)

# Optimize for max sharpe ratio
ef = EfficientFrontier(mu, S)
weights = ef.max_sharpe()
cleaned_weights = ef.clean_weights()
print(cleaned_weights)
ef.portfolio_performance(verbose = True)

"""
Output

OrderedDict([('AMT', 0.17776), ('BXP', 0.09507), ('EQIX', 0.08299), ('PLD', 0.44864), ('CCI', 0.19554)])
Expected annual return: 11.5%
Annual volatility: 31.2%
Sharpe Ratio: 0.30
(0.1145827959207917, 0.31177638162175486, 0.30336741811167384)

"""

"""# Sentiment Analysis"""



import matplotlib.pyplot as plt
from bs4 import BeautifulSoup
import pandas as pd
from urllib.request import urlopen, Request
from nltk.sentiment.vader import SentimentIntensityAnalyzer

finviz_url = 'https://finviz.com/quote.ashx?t='

news_tables={}
# find articles for each real estate ticker
for ticker in realEstate_assets:
    url = finviz_url + ticker

    # request html data from finviz
    req = Request(url=url, headers={'user-agent': 'my-app'})
    response = urlopen(req)

    # parse to isolate table of articles
    html = BeautifulSoup(response, 'html')
    print(html)
    news_table = html.find(id='news-table')
    news_tables[ticker] = news_table

#print(news_tables)

parsed_data = []
import re
for ticker, news_table in news_tables.items():
    # extract data from a tag
    for row in news_table.findAll('tr'):

        # extract url, skip invalid links
        try:
            href = [a['href'] for a in row.find_all('a', href=True)]
            url = href[0]
            '''req = Request(url=link)
            response = urlopen(req)

            html = BeautifulSoup(response, 'html')
            print(html)
            '''
            print(url)

            title = row.a.text
            date_data = row.td.text.split(' ')

            if len(date_data) == 1:
                time = date_data[0]
            else:
                date = date_data[0]
                time = date_data[1]

            parsed_data.append([ticker, date, time, title])
        except:
            print('invalid link')
            continue

print(parsed_data)

import nltk
nltk.download('vader_lexicon')

# scrape other websites and append data to df
df = pd.DataFrame(parsed_data, columns=['ticker', 'date', 'time', 'title'])

vader = SentimentIntensityAnalyzer()

# apply sentiment polarity function to titles
f = lambda title: vader.polarity_scores(title)['compound']
df['compound'] = df['title'].apply(f)
df['date'] = pd.to_datetime(df.date).dt.date

print(df)

# average daily sentiment
mean_df = df.groupby(['ticker', 'date']).mean().unstack()
mean_df = mean_df.xs('compound', axis='columns').transpose()

ax = mean_df.plot(kind='bar', figsize = (32,16))
ax.grid(axis='x')
plt.show()

print('hello')